# AI/LLM/Prompt Engineering Study - Big Board

## Dates

Kick-off: 15 Feb 2025  
Week 1: 22 Feb 2025 - Study plans and potential project ideas
Week 2: 01 Mar 2025 - Read 3 papers on RNNs
Week 3: 08 Mar 2025  
Week 4: 15 Mar 2025  
Week 5: 22 Mar 2025  

## Possible project ideas

1. Article summarising w/ LLMs
2. Image "translator" i.e. predict calories given photo of food
3. Another option is to, depending on my understanding of transformers, RNNs, CNNs, and LTSMs, try to follow and re-implement the transformer paper.

## Study plans

Initial reading, starting from a [Medium post](https://medium.com/@nandinilreddy/understanding-transformers-llms-3794fe228901)

> Just like how Transformers are foundational models to LLMs; RNNs, CNNs, LSTMs, and attention models are foundational models, and each single model contributed to building the Transformer.

1. Check [Ilya 30u30](https://arc.net/folder/84DD0DB7-19AD-4DA5-B0CE-5AA523D3A403) and take a look at papers and blogs that explain RNNs, CNNs, LSTMs, and attention models
    - First topics to cover: RNNs
        - [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
        - [Recurrent Neural Network Regularization](https://arxiv.org/pdf/1409.2329)
        - [Relational recurrent neural networks](https://arxiv.org/pdf/1806.01822)
2. Take a look at transformers
3. Try implementing an LLM project from above
